#!/usr/bin/env python3
"""
Script to fix empty panels in CAMVis summary visualizations.

This script re-extracts heatmap statistics from existing metadata files
and regenerates the summary visualizations with properly populated panels.
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Any, Tuple
import glob
from tqdm import tqdm


class CAMVisSummaryFixer:
    """Fix CAMVis summary visualizations by re-processing existing metadata."""
    
    def __init__(self, session_dir: str):
        """
        Initialize the fixer.
        
        Args:
            session_dir: Path to the CAMVis session directory
        """
        self.session_dir = Path(session_dir)
        self.heatmaps_dir = self.session_dir / "heatmaps"
        self.summaries_dir = self.session_dir / "summaries"
        self.reports_dir = self.session_dir / "reports"
        
        # Ensure directories exist
        self.summaries_dir.mkdir(exist_ok=True)
        self.reports_dir.mkdir(exist_ok=True)
        
        # Class names for AMD grading
        self.class_names = ["Normal", "Early AMD", "Intermediate AMD", "Advanced AMD"]
        
        print(f"Initialized CAMVis Summary Fixer for session: {self.session_dir}")
    
    def extract_heatmap_statistics_from_metadata(self) -> Dict[str, Dict[str, Any]]:
        """
        Extract heatmap statistics from existing metadata files.
        
        Returns:
            Dictionary with split results containing heatmap statistics
        """
        print("ğŸ” Extracting heatmap statistics from metadata files...")
        
        split_results = {}
        
        # Process each split
        for split_dir in self.heatmaps_dir.iterdir():
            if not split_dir.is_dir():
                continue
                
            split_name = split_dir.name
            print(f"  Processing {split_name} split...")
            
            split_stats = {
                'samples_processed': 0,
                'class_counts': {i: 0 for i in range(len(self.class_names))},
                'predictions': [],
                'ground_truths': [],
                'confidences': [],
                'processing_times': [],
                'heatmap_stats': {'mnv': [], 'fluid': [], 'ga': [], 'drusen': []}
            }
            
            # Find all metadata files in this split
            metadata_files = list(split_dir.glob("**/*_all_methods_metadata.json"))
            
            for metadata_file in tqdm(metadata_files, desc=f"  {split_name} metadata"):
                try:
                    with open(metadata_file, 'r') as f:
                        metadata = json.load(f)
                    
                    # Extract basic information
                    true_class = metadata['true_class']
                    pred_class = metadata['predicted_class']
                    confidence = metadata['confidence']
                    sample_id = metadata['sample_id']
                    
                    # Update basic statistics
                    split_stats['samples_processed'] += 1
                    split_stats['class_counts'][true_class] += 1
                    split_stats['predictions'].append(pred_class)
                    split_stats['ground_truths'].append(true_class)
                    split_stats['confidences'].append(confidence)
                    split_stats['processing_times'].append(10.0)  # Placeholder time
                    
                    # Extract heatmap statistics - use gradcam as primary method
                    if 'method_statistics' in metadata and 'gradcam' in metadata['method_statistics']:
                        gradcam_stats = metadata['method_statistics']['gradcam']
                        
                        for inp_name in ['mnv', 'fluid', 'ga', 'drusen']:
                            if inp_name in gradcam_stats:
                                max_activation = gradcam_stats[inp_name]['max_activation']
                                mean_activation = gradcam_stats[inp_name]['mean_activation']
                                
                                split_stats['heatmap_stats'][inp_name].append({
                                    'max': max_activation,
                                    'mean': mean_activation,
                                    'sample_id': sample_id
                                })
                    
                except Exception as e:
                    print(f"    Warning: Error processing {metadata_file}: {e}")
                    continue
            
            # Calculate accuracy
            if split_stats['predictions'] and split_stats['ground_truths']:
                correct = sum(p == g for p, g in zip(split_stats['predictions'], split_stats['ground_truths']))
                accuracy = correct / len(split_stats['predictions'])
                split_stats['accuracy'] = accuracy
            else:
                split_stats['accuracy'] = 0.0
            
            split_results[split_name] = split_stats
            print(f"  âœ“ {split_name}: {split_stats['samples_processed']} samples, {split_stats['accuracy']:.1%} accuracy")
        
        return split_results
    
    def create_summary_visualizations(self, split_results: Dict[str, Dict[str, Any]]):
        """Create comprehensive summary visualizations."""
        print("ğŸ“Š Creating summary visualizations...")
        
        # Set up the plotting style
        plt.style.use('default')
        sns.set_palette("husl")
        
        # 1. Main analysis summary
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('CAM Analysis Summary', fontsize=16, fontweight='bold')
        
        # Accuracy bar plot
        splits = list(split_results.keys())
        accuracies = [split_results[split]['accuracy'] for split in splits]
        
        axes[0, 0].bar(splits, accuracies, color=['skyblue', 'lightgreen', 'salmon'][:len(splits)])
        axes[0, 0].set_title('Accuracy by Dataset Split')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].set_ylim(0, 1)
        for i, acc in enumerate(accuracies):
            axes[0, 0].text(i, acc + 0.01, f'{acc:.1%}', ha='center')
        
        # Class distribution (use first split)
        if split_results:
            split_name = list(split_results.keys())[0]
            class_counts = split_results[split_name]['class_counts']
            
            if any(count > 0 for count in class_counts.values()):
                axes[0, 1].pie(class_counts.values(), labels=self.class_names, autopct='%1.1f%%')
                axes[0, 1].set_title(f'Class Distribution ({split_name})')
            else:
                axes[0, 1].text(0.5, 0.5, 'No data processed', ha='center', va='center')
                axes[0, 1].set_title('Class Distribution - No Data')
        
        # Heatmap activation comparison
        input_names = ['mnv', 'fluid', 'ga', 'drusen']
        avg_activations = {}
        
        for split in splits:
            avg_activations[split] = []
            for inp_name in input_names:
                heatmap_data = split_results[split]['heatmap_stats'].get(inp_name, [])
                if heatmap_data:
                    avg_max = np.mean([h['max'] for h in heatmap_data])
                    avg_activations[split].append(avg_max)
                else:
                    avg_activations[split].append(0)
        
        x = np.arange(len(input_names))
        width = 0.25
        
        for i, split in enumerate(splits):
            axes[1, 0].bar(x + i*width, avg_activations[split], width, label=split)
        
        axes[1, 0].set_xlabel('Input Type')
        axes[1, 0].set_ylabel('Average Max Activation')
        axes[1, 0].set_title('Average Heatmap Activations by Input Type')
        axes[1, 0].set_xticks(x + width)
        axes[1, 0].set_xticklabels([inp.upper() for inp in input_names])
        axes[1, 0].legend()
        
        # Processing time comparison
        proc_times = [np.mean(split_results[split]['processing_times']) if split_results[split]['processing_times'] else 0 
                     for split in splits]
        
        axes[1, 1].bar(splits, proc_times, color=['orange', 'purple', 'brown'][:len(splits)])
        axes[1, 1].set_title('Average Processing Time by Split')
        axes[1, 1].set_ylabel('Time (seconds)')
        for i, time in enumerate(proc_times):
            axes[1, 1].text(i, time + 0.1, f'{time:.1f}s', ha='center')
        
        plt.tight_layout()
        plt.savefig(self.summaries_dir / 'analysis_summary.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ“ Analysis summary saved")
        
        # 2. Detailed heatmap analysis
        self.create_heatmap_analysis_plots(split_results)
    
    def create_heatmap_analysis_plots(self, split_results: Dict[str, Dict[str, Any]]):
        """Create detailed heatmap analysis plots."""
        print("ğŸ“ˆ Creating detailed heatmap analysis...")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Detailed Heatmap Activation Analysis', fontsize=16, fontweight='bold')
        
        input_names = ['mnv', 'fluid', 'ga', 'drusen']
        colors = ['blue', 'green', 'red', 'orange']
        split_names = list(split_results.keys())
        split_colors = {'train': 'blue', 'val': 'green', 'test': 'red'}
        
        for idx, inp_name in enumerate(input_names):
            row, col = idx // 2, idx % 2
            
            # Collect activation data across all splits
            all_max_activations = []
            all_mean_activations = []
            split_labels = []
            
            for split_name, split_data in split_results.items():
                heatmap_data = split_data['heatmap_stats'].get(inp_name, [])
                if heatmap_data:
                    max_acts = [h['max'] for h in heatmap_data]
                    mean_acts = [h['mean'] for h in heatmap_data]
                    
                    all_max_activations.extend(max_acts)
                    all_mean_activations.extend(mean_acts)
                    split_labels.extend([split_name] * len(max_acts))
            
            if all_max_activations:
                # Create scatter plot with different colors for each split
                for split_name in split_names:
                    split_indices = [i for i, label in enumerate(split_labels) if label == split_name]
                    if split_indices:
                        split_max = [all_max_activations[i] for i in split_indices]
                        split_mean = [all_mean_activations[i] for i in split_indices]
                        color = split_colors.get(split_name, colors[split_names.index(split_name) % len(colors)])
                        axes[row, col].scatter(split_mean, split_max, c=color, alpha=0.6, s=20, label=split_name)
                
                axes[row, col].set_xlabel('Mean Activation')
                axes[row, col].set_ylabel('Max Activation')
                axes[row, col].set_title(f'{inp_name.upper()} Heatmap Activations')
                axes[row, col].grid(True, alpha=0.3)
                axes[row, col].legend()
                
                # Add statistics text
                stats_text = f'Samples: {len(all_max_activations)}\n'
                stats_text += f'Max range: [{min(all_max_activations):.3f}, {max(all_max_activations):.3f}]\n'
                stats_text += f'Mean range: [{min(all_mean_activations):.3f}, {max(all_mean_activations):.3f}]'
                axes[row, col].text(0.02, 0.98, stats_text, transform=axes[row, col].transAxes, 
                                  verticalalignment='top', fontsize=8, 
                                  bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            else:
                axes[row, col].text(0.5, 0.5, 'No data available', 
                                  transform=axes[row, col].transAxes, ha='center', va='center')
                axes[row, col].set_title(f'{inp_name.upper()} - No Data')
        
        plt.tight_layout()
        plt.savefig(self.summaries_dir / 'heatmap_detailed_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ“ Detailed heatmap analysis saved")
    
    def update_overall_statistics(self, split_results: Dict[str, Dict[str, Any]]):
        """Update the overall statistics JSON file with corrected heatmap summaries."""
        print("ğŸ“ Updating overall statistics...")
        
        stats = {
            'analysis_timestamp': self.session_dir.name.split('_')[-1] if '_' in self.session_dir.name else "unknown",
            'config_used': 'configs/config_bio.toml',  # Default assumption
            'model_info': {
                'n_classes': len(self.class_names),
                'class_names': self.class_names,
                'backbone': 'efficientnet_b5'  # Default assumption
            },
            'splits': {}
        }
        
        total_samples = 0
        total_correct = 0
        
        for split_name, split_data in split_results.items():
            split_stats = {
                'samples_processed': split_data['samples_processed'],
                'accuracy': split_data['accuracy'],
                'class_distribution': {str(k): v for k, v in split_data['class_counts'].items()},
                'avg_processing_time': np.mean(split_data['processing_times']) if split_data['processing_times'] else 0,
                'avg_confidence': np.mean(split_data['confidences']) if split_data['confidences'] else 0
            }
            
            # Heatmap statistics
            heatmap_summary = {}
            for inp_name, heatmap_data in split_data['heatmap_stats'].items():
                if heatmap_data:
                    max_activations = [h['max'] for h in heatmap_data]
                    mean_activations = [h['mean'] for h in heatmap_data]
                    heatmap_summary[inp_name] = {
                        'avg_max_activation': np.mean(max_activations),
                        'std_max_activation': np.std(max_activations),
                        'avg_mean_activation': np.mean(mean_activations),
                        'std_mean_activation': np.std(mean_activations),
                        'sample_count': len(max_activations)
                    }
            
            split_stats['heatmap_summary'] = heatmap_summary
            stats['splits'][split_name] = split_stats
            
            total_samples += split_data['samples_processed']
            total_correct += int(split_data['accuracy'] * split_data['samples_processed'])
        
        stats['overall'] = {
            'total_samples_processed': total_samples,
            'overall_accuracy': total_correct / total_samples if total_samples > 0 else 0,
        }
        
        # Save updated statistics
        with open(self.reports_dir / 'overall_statistics.json', 'w') as f:
            json.dump(stats, f, indent=2)
        
        print(f"âœ“ Overall statistics updated - {total_samples} samples, {stats['overall']['overall_accuracy']:.1%} accuracy")
        
        return stats
    
    def create_detailed_report(self, split_results: Dict[str, Dict[str, Any]], overall_stats: Dict[str, Any]):
        """Create a detailed text report with corrected heatmap statistics."""
        print("ğŸ“„ Creating detailed report...")
        
        report_lines = [
            "=" * 80,
            "COMPREHENSIVE GRAD-CAM ANALYSIS REPORT (CORRECTED)",
            "=" * 80,
            f"Generated: {overall_stats.get('analysis_timestamp', 'Unknown')}",
            f"Configuration: {overall_stats.get('config_used', 'Unknown')}",
            f"Output Directory: {self.session_dir}",
            "",
            "OVERALL SUMMARY",
            "-" * 40,
            f"Total Samples Processed: {overall_stats['overall']['total_samples_processed']}",
            f"Overall Accuracy: {overall_stats['overall']['overall_accuracy']:.1%}",
            f"Model Classes: {overall_stats['model_info']['n_classes']}",
            f"Backbone: {overall_stats['model_info']['backbone']}",
            ""
        ]
        
        # Add split-specific details
        for split_name, split_stats in overall_stats['splits'].items():
            report_lines.extend([
                f"{split_name.upper()} DATASET ANALYSIS",
                "-" * 40,
                f"Samples Processed: {split_stats['samples_processed']}",
                f"Accuracy: {split_stats['accuracy']:.1%}",
                f"Average Confidence: {split_stats['avg_confidence']:.1%}",
                f"Average Processing Time: {split_stats['avg_processing_time']:.2f}s",
                "",
                "Class Distribution:"
            ])
            
            for class_idx, count in split_stats['class_distribution'].items():
                class_name = self.class_names[int(class_idx)]
                percentage = (count / split_stats['samples_processed'] * 100) if split_stats['samples_processed'] > 0 else 0
                report_lines.append(f"  {class_name}: {count} ({percentage:.1f}%)")
            
            report_lines.extend(["", "Heatmap Statistics:"])
            
            for inp_name, heatmap_data in split_stats['heatmap_summary'].items():
                report_lines.extend([
                    f"  {inp_name.upper()}:",
                    f"    Average Max Activation: {heatmap_data['avg_max_activation']:.4f} Â± {heatmap_data['std_max_activation']:.4f}",
                    f"    Average Mean Activation: {heatmap_data['avg_mean_activation']:.4f} Â± {heatmap_data['std_mean_activation']:.4f}",
                    f"    Samples: {heatmap_data['sample_count']}"
                ])
            
            report_lines.extend(["", "=" * 50, ""])
        
        # Save report
        with open(self.reports_dir / 'detailed_analysis_report.txt', 'w') as f:
            f.write('\n'.join(report_lines))
        
        print("âœ“ Detailed report saved")
    
    def fix_summaries(self):
        """Main method to fix the summary visualizations."""
        print("ğŸ”§ Starting CAMVis summary fix...")
        
        # Step 1: Extract heatmap statistics from metadata
        split_results = self.extract_heatmap_statistics_from_metadata()
        
        if not any(split_results.values()):
            print("âŒ No data found to process!")
            return
        
        # Step 2: Create corrected visualizations
        self.create_summary_visualizations(split_results)
        
        # Step 3: Update statistics
        overall_stats = self.update_overall_statistics(split_results)
        
        # Step 4: Create corrected report
        self.create_detailed_report(split_results, overall_stats)
        
        print("âœ… CAMVis summary fix completed successfully!")
        print(f"ğŸ“ Check {self.summaries_dir} for updated visualizations")


def main():
    """Main function to run the CAMVis summary fixer."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Fix CAMVis summary visualizations")
    parser.add_argument('--session_dir', type=str, 
                       default='CAMVis/session_20251114_172306',
                       help='Path to CAMVis session directory')
    
    args = parser.parse_args()
    
    # Initialize and run the fixer
    fixer = CAMVisSummaryFixer(args.session_dir)
    fixer.fix_summaries()


if __name__ == '__main__':
    main()